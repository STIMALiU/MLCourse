ROCResults <- ROC(PredictLogistic, thresholds = seq(0.00000001,0.1,length = 10000), XTest, betaHat)
plot(ROCResults[,1],ROCResults[,2])
ROCResults <- ROC(PredictLogistic, thresholds = seq(0.00000001,0.1,length = 10000), XTest, betaHat)
plot(ROCResults[,1],ROCResults[,2])
ROCResults
results <- PredictLogistic(threshold = 0.00000000001, XTest,betaHat);c(results$precision, results$recall)
results <- PredictLogistic(threshold = 0.00000001, XTest,betaHat);c(results$precision, results$recall)
# Predition function
PredictLogistic <- function(threshold = 0.5, XTest, betaHat){
linFunc = XTest%*%betaHat # matrix product
thetaVect = exp(linFunc)/(1+exp(linFunc))
results = list()
results$probs <- thetaVect
results$preds <- ifelse(thetaVect>threshold,1,0)
results$confusionMatrix <- table(results$preds,yTest)
results$accuracy = sum(diag(results$confusionMatrix))/dim(XTest)[1] # What proportion of notes were correctly classified?
results$precision = results$confusionMatrix[2,2]/sum(results$confusionMatrix[2,]) # Out of those selected (marked as fraud) what proportion were right?
results$recall = results$confusionMatrix[2,2]/sum(results$confusionMatrix[,2])# What proportion of frauds were detected? Sensitivity. True positive rate.
results$FPR = results$confusionMatrix[2,1]/sum(results$confusionMatrix[,1]) # False Positive Rate
return(results)
}
results <- PredictLogistic(threshold = 0.00000001, XTest,betaHat);c(results$FPR, results$recall)
results <- PredictLogistic(threshold = 0.00000000001, XTest,betaHat);c(results$precision, results$recall);results$confusionMatrix
spam
dim(spam)
head(spam)
dim(spam)
library(ElemStatLearn)
data <- spam
SelectTraining <- sample(1:dim(data)[1], size = 3000, replace = FALSE)
y <- data[,58]
ifelse(y="spam",1,0)
y[1:4]
y[1] ==spam
y[1] =="spam""
""
y[1] =="spam"
y == spam
y == "spam"
ifelse(y=="spam",1,0)
y <- data[,58]
y <- ifelse(y=="spam",1,0)
X <- as.matrix(cbind(1,data[,1:57])) # Adding a column of ones for the intercept
nPara <- dim(X)[2]       # Number of covariates incl intercept
yTrain <- y[SelectTraining]
yTest <- y[-SelectTraining]
XTrain <- X[SelectTraining,]
XTest <- X[-SelectTraining,]
yTrain
initPar <- matrix(0,nPara,1)
optimResults <- optim(initPar, LogLik, gr = NULL, yTrain, XTrain, control=list(fnscale=-1))
betaHat <- optimResults$par
# Predition function
PredictLogistic <- function(threshold = 0.5, XTest, betaHat){
linFunc = XTest%*%betaHat # matrix product
thetaVect = exp(linFunc)/(1+exp(linFunc))
results = list()
results$probs <- thetaVect
results$preds <- ifelse(thetaVect>threshold,1,0)
results$confusionMatrix <- table(results$preds,yTest)
results$accuracy = sum(diag(results$confusionMatrix))/dim(XTest)[1] # What proportion of notes were correctly classified?
results$precision = results$confusionMatrix[2,2]/sum(results$confusionMatrix[2,]) # Out of those selected (marked as fraud) what proportion were right?
results$recall = results$confusionMatrix[2,2]/sum(results$confusionMatrix[,2])# What proportion of frauds were detected? Sensitivity. True positive rate.
results$FPR = results$confusionMatrix[2,1]/sum(results$confusionMatrix[,1]) # False Positive Rate
return(results)
}
# Predicting the test set and evaluating the results with threshold = 0.5
results <- PredictLogistic(threshold = 0.5, XTest,betaHat)
results
results <- PredictLogistic(threshold = 0.9, XTest,betaHat)
results
yTest
results <- PredictLogistic(threshold = 0.5, XTest,betaHat)
results
ROCResults <- ROC(PredictLogistic, thresholds = seq(0.001,0.999,length = 10000), XTest, betaHat)
plot(ROCResults[,1],ROCResults[,2])
seq(0.001,0.999,length = 10000)
ROCResults <- ROC(PredictLogistic, thresholds = seq(0.001,0.999,length = 10000), XTest, betaHat)
betaHat
XTest
# Predition function
PredictLogistic <- function(threshold = 0.5, yTest, XTest, betaHat){
linFunc = XTest%*%betaHat # matrix product
thetaVect = exp(linFunc)/(1+exp(linFunc))
results = list()
results$probs <- thetaVect
results$preds <- ifelse(thetaVect>threshold,1,0)
results$confusionMatrix <- table(results$preds,yTest)
results$accuracy = sum(diag(results$confusionMatrix))/dim(XTest)[1] # What proportion of notes were correctly classified?
results$precision = results$confusionMatrix[2,2]/sum(results$confusionMatrix[2,]) # Out of those selected (marked as fraud) what proportion were right?
results$recall = results$confusionMatrix[2,2]/sum(results$confusionMatrix[,2])# What proportion of frauds were detected? Sensitivity. True positive rate.
results$FPR = results$confusionMatrix[2,1]/sum(results$confusionMatrix[,1]) # False Positive Rate
return(results)
}
# Predicting the test set and evaluating the results with threshold = 0.5
results <- PredictLogistic(threshold = 0.5, yTest, XTest, betaHat)
ROC <-function(PredictFunction, thresholds, ...){
# PredictFunction is a function that returns a list 'results' with elements results$precision and results$recall
# First element of PredictFunction must be threshold, the other arguments can be anything needed for that function (hence the  ... argument)
nThres <- length(thresholds)
precision <- rep(NA,nThres)
recall <- rep(NA,nThres)
count <- 0
for (threshold in thresholds){
count = count + 1
results <- PredictFunction(threshold = threshold, ...)
precision[count] <- results$precision
recall[count] <- results$recall # This is also True Positive Rate (TPR)
FPR <- results$confusionMatrix[2,1]/sum(results$confusionMatrix[,1]) # False Positive Rate
}
return(cbind(FPR,recall))
}
ROCResults <- ROC(PredictLogistic, thresholds = seq(0.001,0.999,length = 10000), XTest, betaHat)
ROCResults <- ROC(PredictLogistic, thresholds = seq(0.001,0.999,length = 10000), yTest, XTest, betaHat)
# Logistic regression for banknote fraud data from http://archive.ics.uci.edu/ml/datasets/banknote+authentication#
# Author: Mattias Villani, Statistics and Machine Learning, Linkoping University, Sweden. e-mail: mattias.villani@liu.se
# Defining the log-likelihood function
LogLik <- function(betaVect,y,X){
linFunc = X%*%betaVect # matrix product
thetaVect = exp(linFunc)/(1+exp(linFunc))
logLikelihood <- sum(y*log(thetaVect) + (1-y)*log(1-thetaVect))
}
# Reading in fraud data from file
data <- read.csv('~/Dropbox/Teaching/ProbStatUProg/Data/banknoteFraud.csv', header = FALSE)
names(data) <- c("varWave","skewWave","kurtWave","entropyWave","fraud")
SelectTraining <- sample(1:dim(data)[1], size = 1000, replace = FALSE)
y <- data[,5]
X <- as.matrix(cbind(1,data[,1:4])) # Adding a column of ones for the intercept
nPara <- dim(X)[2]       # Number of covariates incl intercept
yTrain <- y[SelectTraining]
yTest <- y[-SelectTraining]
XTrain <- X[SelectTraining,]
XTest <- X[-SelectTraining,]
# Optimize to the find the ML estimates. control = list(fnscale=-1) puts a minus sign in front of LogLik
# We need this since optim minimizes, not maximizes.
initPar <- matrix(0,nPara,1)
optimResults <- optim(initPar, LogLik, gr = NULL, yTrain, XTrain, control=list(fnscale=-1))
betaHat <- optimResults$par
# Predition function
PredictLogistic <- function(threshold = 0.5, XTest, betaHat){
linFunc = XTest%*%betaHat # matrix product
thetaVect = exp(linFunc)/(1+exp(linFunc))
results = list()
results$probs <- thetaVect
results$preds <- ifelse(thetaVect>threshold,1,0)
results$confusionMatrix <- table(results$preds,yTest)
results$accuracy = sum(diag(results$confusionMatrix))/dim(XTest)[1] # What proportion of notes were correctly classified?
results$precision = results$confusionMatrix[2,2]/sum(results$confusionMatrix[2,]) # Out of those selected (marked as fraud) what proportion were right?
results$recall = results$confusionMatrix[2,2]/sum(results$confusionMatrix[,2])# What proportion of frauds were detected? Sensitivity. True positive rate.
results$FPR = results$confusionMatrix[2,1]/sum(results$confusionMatrix[,1]) # False Positive Rate
return(results)
}
# Predicting the test set and evaluating the results with threshold = 0.5
results <- PredictLogistic(threshold = 0.5, XTest,betaHat)
dim(XTest)
dim(XTrain)
results <- PredictLogistic(threshold = 0.5, XTest,betaHat)
results
source('~/Dropbox/Teaching/ProbStatUProg/Code/IllustrationSurvivalAnalysis.R')
hist(alphaBoot, 30, xlim = c(0.5,3), main = "Bootstrap approx of sampling distr. of alphaEst", freq = FALSE)
points(alphaEstML,0,col='red')
points(1,0,col='blue')
sdAlphaBoot <- sd(alphaBoot)
Test <- (alphaEstML - 1)/sdAlphaBoot
pValueAlphaTest <- pnorm(abs(Test), lower.tail = FALSE)
ppValueAlphaTest # Reject!
hist(alphaBoot, 30, xlim = c(0.5,3), main = "Bootstrap approx of sampling distr. of alphaEst", freq = FALSE)
points(alphaEstML,0,col='red')
points(1,0,col='blue')
sdAlphaBoot <- sd(alphaBoot)
Test <- (alphaEstML - 1)/sdAlphaBoot
pValueAlphaTest <- pnorm(abs(Test), lower.tail = FALSE)
pValueAlphaTest # Reject!
# 95 K.I. fÃ¶r alpha
alphaLevel = 0.05;
zAlphaHalf <- qnorm(alphaLevel/2, lower.tail = FALSE)
c(alphaEstML - zAlphaHalf*sdAlphaBoot,alphaEstML + zAlphaHalf*sdAlphaBoot) # Normal approx
quantile(alphaBoot, probs = c(alphaLevel/2,1-alphaLevel/2)) # Bootstrap
# fit an gamma distribution
hist(cancer$time, freq=FALSE)
lines(0:1100,dexp(0:1100,lambdaEstExp),col="blue",lwd=3)
lines(0:1100,dgamma(0:1100,alphaEstMom,lambdaEstMom),col="red",lwd=3)
lines(0:1100,dgamma(0:1100,alphaEstML,lambdaEstML),col="green",lwd=3)
title('Fit of Exp and Gamma distribution')
# Chi-square goodness of fit test for the gamma model
ProbsModel = diff(c(0,pgamma(c(100,200,300,400,500,600,800),MLest[1],MLest[2]),1))
ExpectedCounts = n*ProbsModel
Chi2 <- sum(((ObservedCounts - ExpectedCounts)^2)/ExpectedCounts)
Df = length(ExpectedCounts) - 3 # N - d - 1, where d=2 is the number of estimate model parameters
pValue <- pchisq(Chi2, df = Df, lower.tail = FALSE)
Chi2
pValue
plot(seq(0,40,by=0.1),dchisq(seq(0,40,by=0.1),df=Df), type ="l", lwd = 3, main = 'Chi2 null distribution',
xlab = "Chi2-statistic", ylab="density")
points(Chi2,0,col='red')
# Censoring!
# Fit Gamma by ML with censoring
censored <- (status==1)
LogLik <- function(theta, x, censored){
logL <- sum(dgamma(x[censored==F], exp(theta[1]), exp(theta[2]), log = TRUE)) +
sum(pgamma(x[censored==T], exp(theta[1]), exp(theta[2]), log.p = TRUE, lower.tail = FALSE))
return(logL)
}
initPar <- c(log(alphaEst),log(lambdaEst)) # Initial values from moment estimators
optimResults <- optim(initPar, LogLik, gr = NULL, x = cancer$time, censored = censored, control=list(fnscale=-1))
MLestCensoring <- exp(optimResults$par)
MLestCensoring
# fit an gamma distribution
hist(cancer$time, freq=FALSE)
lines(0:1100,dexp(0:1100,1/mean(cancer$time)),col="blue",lwd=3)
lines(0:1100,dgamma(0:1100,alphaEstMom,lambdaEstMom),col="red",lwd=3)
lines(0:1100,dgamma(0:1100,alphaEstML,alphaEstML),col="green",lwd=3)
lines(0:1100,dgamma(0:1100,MLestCensoring[1],MLestCensoring[2]),col="yellow",lwd=3)
title('Fit of Exp and Gamma distribution')
# Regression survival time explained by age
plot(cancer$age,cancer$time)
lm1 <- lm(time ~ age, data = cancer)
summary(lm1)
abline(lm1)
plot(ph.ecog,time)
lm2 <- lm(time ~ ph.ecog, data = cancer)
summary(lm2)
abline(lm2)
lm3 <- lm(time ~ age + sex + ph.ecog + wt.loss, data = cancer)
summary(lm3)
glm1 <- glm(time ~ age + sex + ph.ecog + wt.loss, family = Gamma, data = cancer)
summary(glm1)
# Survival regression. Exponentially distributed survival time.
# Censoring is accounted for.
survival1 <- survreg(Surv(time, status) ~ age + sex + ph.ecog + wt.loss,
data = cancer, dist="exponential")
summary(survival1)
dev.off()
source('~/Dropbox/Teaching/IntroToML/Introduction/Code/fraudDetect.R')
# Logistic regression for spam data from the book Elements of Statistical Learning
# Author: Mattias Villani, Statistics and Machine Learning, Linkoping University, Sweden. e-mail: mattias.villani@liu.se
# Defining the log-likelihood function
LogLik <- function(betaVect,y,X){
linFunc = X%*%betaVect # matrix product
thetaVect = exp(linFunc)/(1+exp(linFunc))
logLikelihood <- sum(y*log(thetaVect) + (1-y)*log(1-thetaVect))
}
# Reading in spam data from the ElemStatLearn package
#install.packages("ElemStatLearn")
library(ElemStatLearn)
data <- spam
SelectTraining <- sample(1:dim(data)[1], size = 3000, replace = FALSE)
y <- data[,58]
y <- ifelse(y=="spam",1,0)
X <- as.matrix(cbind(1,data[,1:57])) # Adding a column of ones for the intercept
nPara <- dim(X)[2]       # Number of covariates incl intercept
yTrain <- y[SelectTraining]
yTest <- y[-SelectTraining]
XTrain <- X[SelectTraining,]
XTest <- X[-SelectTraining,]
# Optimize to the find the ML estimates. control = list(fnscale=-1) puts a minus sign in front of LogLik
# We need this since optim minimizes, not maximizes.
initPar <- matrix(0,nPara,1)
optimResults <- optim(initPar, LogLik, gr = NULL, yTrain, XTrain, control=list(fnscale=-1))
betaHat <- optimResults$par
# Predition function
PredictLogistic <- function(threshold = 0.5, yTest, XTest, betaHat){
linFunc = XTest%*%betaHat # matrix product
thetaVect = exp(linFunc)/(1+exp(linFunc))
results = list()
results$probs <- thetaVect
results$preds <- ifelse(thetaVect>threshold,1,0)
results$confusionMatrix <- table(results$preds,yTest)
results$accuracy = sum(diag(results$confusionMatrix))/dim(XTest)[1] # What proportion of notes were correctly classified?
results$precision = results$confusionMatrix[2,2]/sum(results$confusionMatrix[2,]) # Out of those selected (marked as fraud) what proportion were right?
results$recall = results$confusionMatrix[2,2]/sum(results$confusionMatrix[,2])# What proportion of frauds were detected? Sensitivity. True positive rate.
results$FPR = results$confusionMatrix[2,1]/sum(results$confusionMatrix[,1]) # False Positive Rate
return(results)
}
# Predicting the test set and evaluating the results with threshold = 0.5
results <- PredictLogistic(threshold = 0.5, yTest, XTest, betaHat)
ROC <-function(PredictFunction, thresholds, ...){
# PredictFunction is a function that returns a list 'results' with elements results$precision and results$recall
# First element of PredictFunction must be threshold, the other arguments can be anything needed for that function (hence the  ... argument)
nThres <- length(thresholds)
precision <- rep(NA,nThres)
recall <- rep(NA,nThres)
count <- 0
for (threshold in thresholds){
count = count + 1
results <- PredictFunction(threshold = threshold, ...)
precision[count] <- results$precision
recall[count] <- results$recall # This is also True Positive Rate (TPR)
FPR <- results$confusionMatrix[2,1]/sum(results$confusionMatrix[,1]) # False Positive Rate
}
return(cbind(FPR,recall))
}
ROCResults <- ROC(PredictLogistic, thresholds = seq(0.001,0.999,length = 10000), yTest, XTest, betaHat)
plot(ROCResults[,1],ROCResults[,2])
library(e1071)
library(rpart)
fitSVM <- svm(y = yTrain, x = xTrain)
fitSVM <- svm(y = yTrain, x = XTrain)
predAllSVM <- predict(fitSVM, xTest, type = "class")
predAllSVM <- predict(fitSVM, XTest, type = "class")
confusSVM <- table(as.matrix(predAllSVM),as.matrix(yTest))
accuracySVM <- sum(diag(confusSVM))/sum(confusSVM)
confusSVM
svm
?svm
fitSVM <- svm(y = yTrain, x = XTrain, type ="C-classification")
predAllSVM <- predict(fitSVM, XTest, type = "class")
confusSVM <- table(as.matrix(predAllSVM),as.matrix(yTest))
accuracySVM <- sum(diag(confusSVM))/sum(confusSVM)
confusSVM
?predict
?e1071::predict.svm
accuracySVM
recallSVM <- confusSVM[2,2]/sum(confusSVM[,2])
precisionSVM <- confusSVM[2,2]/sum(confusSVM[2,])
c(accuracySVM,recallSVM,precisionSVM)
?naiveBayes
fitNB <- svm(y = yTrain, x = XTrain, type ="C-classification")
fitNB <- svm(y = yTrain, x = XTrain)
predAllNB <- predict(fitNB, XTest, type = "class")
?predict.naiveBayes
fitNB <- svm(y = yTrain, x = XTrain)
predAllNB <- predict(fitNB, XTest, type = "class")
confusNB <- table(as.matrix(predAllNB),as.matrix(yTest))
accuracyNB <- sum(diag(confusNB))/sum(confusNB)
recallNB <- confusNB[2,2]/sum(confusNB[,2])
precisionNB <- confusNB[2,2]/sum(confusNB[2,])
c(accuracyNB,recallNB,precisionNB)
fitNB <- svm(y = yTrain, x = XTrain)
predAllNB <- predict(fitNB, XTest, type = "class")
confusNB <- table(as.matrix(predAllNB),as.matrix(yTest))
accuracyNB <- sum(diag(confusNB))/sum(confusNB)
recallNB <- confusNB[2,2]/sum(confusNB[,2])
precisionNB <- confusNB[2,2]/sum(confusNB[2,])
c(accuracyNB,recallNB,precisionNB)
fitNB
fitNB <- naiveBayes(y = yTrain, x = XTrain)
predAllNB <- predict(fitNB, XTest, type = "class")
confusNB <- table(as.matrix(predAllNB),as.matrix(yTest))
accuracyNB <- sum(diag(confusNB))/sum(confusNB)
recallNB <- confusNB[2,2]/sum(confusNB[,2])
precisionNB <- confusNB[2,2]/sum(confusNB[2,])
c(accuracyNB,recallNB,precisionNB)
fitNB
predAllNB
predAllNB <- predict(fitNB, XTest)
predAllNB
fitNB <- naiveBayes(y = yTrain, x = XTrain)
fitNB
predAllNB <- predict(fitNB, XTest, type = "class")
confusNB <- table(as.matrix(predAllNB),as.matrix(yTest))
accuracyNB <- sum(diag(confusNB))/sum(confusNB)
recallNB <- confusNB[2,2]/sum(confusNB[,2])
precisionNB <- confusNB[2,2]/sum(confusNB[2,])
c(accuracyNB,recallNB,precisionNB)
confusNB
predAllNB
confusNB <- NA
predAllNB <- predict(fitNB, XTest, type = "class")
confusNB <- table(as.matrix(predAllNB),as.matrix(yTest))
accuracyNB <- sum(diag(confusNB))/sum(confusNB)
recallNB <- confusNB[2,2]/sum(confusNB[,2])
precisionNB <- confusNB[2,2]/sum(confusNB[2,])
c(accuracyNB,recallNB,precisionNB)
confusNB
# Logistic regression for spam data from the book Elements of Statistical Learning
# Author: Mattias Villani, Statistics and Machine Learning, Linkoping University, Sweden. e-mail: mattias.villani@liu.se
# Defining the log-likelihood function
LogLik <- function(betaVect,y,X){
linFunc = X%*%betaVect # matrix product
thetaVect = exp(linFunc)/(1+exp(linFunc))
logLikelihood <- sum(y*log(thetaVect) + (1-y)*log(1-thetaVect))
}
# Reading in spam data from the ElemStatLearn package
#install.packages("ElemStatLearn")
library(ElemStatLearn)
data <- spam
SelectTraining <- sample(1:dim(data)[1], size = 3000, replace = FALSE)
y <- data[,58]
y <- ifelse(y=="spam",1,0)
X <- as.matrix(cbind(1,data[,1:57])) # Adding a column of ones for the intercept
nPara <- dim(X)[2]       # Number of covariates incl intercept
yTrain <- y[SelectTraining]
yTest <- y[-SelectTraining]
XTrain <- X[SelectTraining,]
XTest <- X[-SelectTraining,]
# Optimize to the find the ML estimates. control = list(fnscale=-1) puts a minus sign in front of LogLik
# We need this since optim minimizes, not maximizes.
initPar <- matrix(0,nPara,1)
optimResults <- optim(initPar, LogLik, gr = NULL, yTrain, XTrain, control=list(fnscale=-1))
betaHat <- optimResults$par
# Predition function
PredictLogistic <- function(threshold = 0.5, yTest, XTest, betaHat){
linFunc = XTest%*%betaHat # matrix product
thetaVect = exp(linFunc)/(1+exp(linFunc))
results = list()
results$probs <- thetaVect
results$preds <- ifelse(thetaVect>threshold,1,0)
results$confusionMatrix <- table(results$preds,yTest)
results$accuracy = sum(diag(results$confusionMatrix))/dim(XTest)[1] # What proportion of notes were correctly classified?
results$precision = results$confusionMatrix[2,2]/sum(results$confusionMatrix[2,]) # Out of those selected (marked as fraud) what proportion were right?
results$recall = results$confusionMatrix[2,2]/sum(results$confusionMatrix[,2])# What proportion of frauds were detected? Sensitivity. True positive rate.
results$FPR = results$confusionMatrix[2,1]/sum(results$confusionMatrix[,1]) # False Positive Rate
return(results)
}
# Predicting the test set and evaluating the results with threshold = 0.5
results <- PredictLogistic(threshold = 0.5, yTest, XTest, betaHat)
ROC <-function(PredictFunction, thresholds, ...){
# PredictFunction is a function that returns a list 'results' with elements results$precision and results$recall
# First element of PredictFunction must be threshold, the other arguments can be anything needed for that function (hence the  ... argument)
nThres <- length(thresholds)
precision <- rep(NA,nThres)
recall <- rep(NA,nThres)
count <- 0
for (threshold in thresholds){
count = count + 1
results <- PredictFunction(threshold = threshold, ...)
precision[count] <- results$precision
recall[count] <- results$recall # This is also True Positive Rate (TPR)
FPR <- results$confusionMatrix[2,1]/sum(results$confusionMatrix[,1]) # False Positive Rate
}
return(cbind(FPR,recall))
}
ROCResults <- ROC(PredictLogistic, thresholds = seq(0.001,0.999,length = 10000), yTest, XTest, betaHat)
plot(ROCResults[,1],ROCResults[,2])
# Logistic regression for spam data from the book Elements of Statistical Learning
# Author: Mattias Villani, Statistics and Machine Learning, Linkoping University, Sweden. e-mail: mattias.villani@liu.se
# Defining the log-likelihood function
LogLik <- function(betaVect,y,X){
linFunc = X%*%betaVect # matrix product
thetaVect = exp(linFunc)/(1+exp(linFunc))
logLikelihood <- sum(y*log(thetaVect) + (1-y)*log(1-thetaVect))
}
# Reading in spam data from the ElemStatLearn package
#install.packages("ElemStatLearn")
library(ElemStatLearn)
data <- spam
SelectTraining <- sample(1:dim(data)[1], size = 3000, replace = FALSE)
y <- data[,58]
y <- ifelse(y=="spam",1,0)
X <- as.matrix(cbind(1,data[,1:57])) # Adding a column of ones for the intercept
nPara <- dim(X)[2]       # Number of covariates incl intercept
yTrain <- y[SelectTraining]
yTest <- y[-SelectTraining]
XTrain <- X[SelectTraining,]
XTest <- X[-SelectTraining,]
# Optimize to the find the ML estimates. control = list(fnscale=-1) puts a minus sign in front of LogLik
# We need this since optim minimizes, not maximizes.
initPar <- matrix(0,nPara,1)
optimResults <- optim(initPar, LogLik, gr = NULL, yTrain, XTrain, control=list(fnscale=-1))
betaHat <- optimResults$par
# Predition function
PredictLogistic <- function(threshold = 0.5, yTest, XTest, betaHat){
linFunc = XTest%*%betaHat # matrix product
thetaVect = exp(linFunc)/(1+exp(linFunc))
results = list()
results$probs <- thetaVect
results$preds <- ifelse(thetaVect>threshold,1,0)
results$confusionMatrix <- table(results$preds,yTest)
results$accuracy = sum(diag(results$confusionMatrix))/dim(XTest)[1] # What proportion of notes were correctly classified?
results$precision = results$confusionMatrix[2,2]/sum(results$confusionMatrix[2,]) # Out of those selected (marked as fraud) what proportion were right?
results$recall = results$confusionMatrix[2,2]/sum(results$confusionMatrix[,2])# What proportion of frauds were detected? Sensitivity. True positive rate.
results$FPR = results$confusionMatrix[2,1]/sum(results$confusionMatrix[,1]) # False Positive Rate
return(results)
}
# Predicting the test set and evaluating the results with threshold = 0.5
results <- PredictLogistic(threshold = 0.5, yTest, XTest, betaHat)
results$accuracy
accuracySVM
source('~/Dropbox/Teaching/IntroToML/Introduction/Code/FishDensityEstimation.R')
# Kernel density estimators
par(mfrow = c(2,2))
hist(fLength,40,  main = 'Default bandwidth', xlab ="fLength", freq = FALSE)
lines(density(fLength, adjust = 1), col = "red", lwd= 2)
hist(fLength,40,  main = 'Twice default bandwidth', xlab ="fLength", freq = FALSE)
lines(density(fLength, adjust = 2), col = "red", lwd= 2)
hist(fLength,40,  main = 'Half default bandwidth', xlab ="fLength", freq = FALSE)
lines(density(fLength, adjust = 0.5), col = "red", lwd= 2)
hist(fLength,40,  main = 'Quarter default bandwidth', xlab ="fLength", freq = FALSE)
lines(density(fLength, adjust = 0.25), col = "red", lwd= 2)
######## k Nearest Neighbours (kNN) ##########
par(mfrow = c(1,2))
kNNDensEst <- rep(0,length(xGrid))
sortedData <- sort(fLength+0.000000*runif(length(fLength)))
N <- length(fLength)
K <- 15
count <- 0
for (i in xGrid){
count <- count + 1
dist <- (sortedData-i)^2 # Distance
indexKNeighbours <- order(dist)[1:K]
V <- max(sortedData[indexKNeighbours])-min(sortedData[indexKNeighbours])
print(V)
kNNDensEst[count] <- K/(V*N)
}
hist(fLength, 40, freq = FALSE, ylim = c(0,0.1), main = 'k=15')
lines(xGrid,kNNDensEst,type="l", col='red', lwd = 2)
kNNDensEst <- rep(0,length(xGrid))
sortedData <- sort(fLength+0.000000*runif(length(fLength)))
N <- length(fLength)
K <- 25
count <- 0
for (i in xGrid){
count <- count + 1
dist <- (sortedData-i)^2 # Distance
indexKNeighbours <- order(dist)[1:K]
V <- max(sortedData[indexKNeighbours])-min(sortedData[indexKNeighbours])
print(V)
kNNDensEst[count] <- K/(V*N)
}
hist(fLength, 40, freq = FALSE, ylim = c(0,0.1), main = 'k=25')
lines(xGrid,kNNDensEst,type="l", col='red', lwd = 2)
